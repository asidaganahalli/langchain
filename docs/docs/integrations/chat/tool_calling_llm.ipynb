{
 "cells": [
  {
   "cell_type": "raw",
   "id": "23347b4f3990a67f",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Tool Calling LLM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22226fd146be3f7d",
   "metadata": {},
   "source": [
    "# Tool Calling LLM\n",
    "\n",
    "This notebook provides a quick overview for getting started with Tool Calling LLM for [chat models](/docs/concepts/#chat-models). For detailed documentation of all ToolCallingLLM features and configurations head to the [API reference](https://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.tool_calling_llm.ToolCallingLLM.html).\n",
    "\n",
    "Tool Calling LLM is a python mixin that lets you add [tool calling capabilities](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) effortlessly to [`LangChain Chat Models`](https://python.langchain.com/v0.2/docs/integrations/chat/) that don't yet support tool/function calling natively. \n",
    "Simply create a new chat model class with `ToolCallingLLM` and your favorite chat model to get started.  \n",
    "\n",
    "With `ToolCallingLLM` you also get access to [`.with_structured_output()`](https://python.langchain.com/v0.2/docs/how_to/structured_output/) which will allow you to return structured data from your model.  \n",
    "\n",
    "At this time, `ToolCallingLLM` has been tested to work with [`ChatNVIDIA`](https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/) and [`ChatLiteLLM`](https://python.langchain.com/v0.2/docs/integrations/chat/litellm/) with Ollama provider.  \n",
    "\n",
    "The [`OllamaFunctions`](https://python.langchain.com/v0.2/docs/integrations/chat/ollama_functions/) which was the original inspiration for this effort has also been ported over to use `ToolCallingLLM`. The code for `ToolCallingLLM` was abstracted out of `OllamaFunctions` to allow it to be reused with other non tool calling Chat Models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |\n",
    "|:-----:|:-------:|:-----:|:------------:|:----------:|:-----------------:|:--------------:|\n",
    "| [ToolCallingLLM](https://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.tool_calling_llm.ToolCallingLLM.html) | [langchain-experimental](https://api.python.langchain.com/en/latest/openai_api_reference.html) | ✅ | ❌ | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-experimental?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-experimental?style=flat-square&label=%20) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](/docs/how_to/tool_calling/) | [Structured output](/docs/how_to/structured_output/) | JSON mode | Image input | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "To access `ToolCallingLLM` you will need to install `langchain-experimental` integration package. You will also need to install integration packages necessary for the chat model you wish to extend using `ToolCallingLLM`.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Any credentials requirements for your chat model will also be applicable when extending with `ToolCallingLLM`.\n",
    "\n",
    "### Installation\n",
    "\n",
    "The `ToolCallingLLM` class lives in the `langchain-experimental` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdfaba055a5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdb0424a20edda",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "`ToolCallingLLM` cannot be used directly. In order to use it, you start with a chat model which doesn't already support tool calling, and extend it with with `ToolCallingLLM`. See below example of extending `ChatLiteLLM` with tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea6adb9641f3174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:53:00.594976Z",
     "start_time": "2024-06-20T03:52:59.695540Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Example implementation using LiteLLM\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_experimental.llms.tool_calling_llm import ToolCallingLLM\n",
    "\n",
    "\n",
    "class LiteLLMFunctions(ToolCallingLLM, ChatLiteLLM):\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"litellm_functions\"\n",
    "\n",
    "\n",
    "llm = LiteLLMFunctions(model=\"ollama/phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539b84e5237b714",
   "metadata": {},
   "source": "## Invocation"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e00cf9e5020e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:55:08.323955Z",
     "start_time": "2024-06-20T03:55:06.412805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer en français.\", id='run-012096e5-caa5-4dda-8863-09ffa30551d7-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2590cc4eef5d8bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:55:11.168049Z",
     "start_time": "2024-06-20T03:55:11.164453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime programmer en français.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f66127e23ea13c",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42835f0194ea6424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:56:52.250080Z",
     "start_time": "2024-06-20T03:56:49.382015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Es freut mich, dass Sie sich für das Programmieren interessieren! Das ist eine faszinierende Fähigkeit.', id='run-3f513ffc-b866-4caf-b700-16ff977fd26c-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e2213e98ef14f",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "ToolCallingLLM was designed to allow you to use tool calling features with Chat Models that don't already support that natively.\n",
    "\n",
    "### ToolCallingLLM.bind_tools()\n",
    "\n",
    "With `ToolCallingLLM.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to a tool definition schemas, which looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2077cf1eee2af71a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:58:35.528395Z",
     "start_time": "2024-06-20T03:58:35.524237Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([GetWeather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7cb900aff2e4af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:58:54.250696Z",
     "start_time": "2024-06-20T03:58:51.278187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', id='run-412dd250-1c87-4ea9-8dc1-70eb848c5899-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_fc9a5ceb5ee3483f9dff5a29013d54f9'}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather like in San Francisco\",\n",
    ")\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687382c464f88c0e",
   "metadata": {},
   "source": [
    "### AIMessage.tool_calls\n",
    "Notice that the AIMessage has a `tool_calls` attribute. This contains in a standardized ToolCall format that is model-provider agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d107347f42a3ca1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:59:46.232568Z",
     "start_time": "2024-06-20T03:59:46.228879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetWeather',\n",
       "  'args': {'location': 'San Francisco, CA'},\n",
       "  'id': 'call_fc9a5ceb5ee3483f9dff5a29013d54f9'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830b66091490b3a",
   "metadata": {},
   "source": "For more on binding tools and tool call outputs, head to the [tool calling](/docs/how_to/function_calling) docs."
  },
  {
   "cell_type": "markdown",
   "id": "db8def1a36ae74d9",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "ToolCallingLLM also supports structured outputs.\n",
    "\n",
    "With `ToolCallingLLM.with_structured_output()` you can specify a Pydantic class or a json schema to define the structure of the output you desire from the llm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e176614b5b7da538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T04:09:14.114872Z",
     "start_time": "2024-06-20T04:09:06.713924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='A cat walks into a bar and says...', punchline=\"Meow! I'll have the beer, but no cheese.\", rating=8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06678661b0cfef2",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all ToolCallingLLM features and configurations head to the API reference: https://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.tool_calling_llm.ToolCallingLLM.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
