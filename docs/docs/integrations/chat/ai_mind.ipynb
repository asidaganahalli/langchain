{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f094648-0843-4951-8a48-fedf250c2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "sidebar_label: AIMind\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa0e2-1085-4889-b622-6b530c051885",
   "metadata": {},
   "source": [
    "# ChatAIMind\n",
    "\n",
    "This notebook provides a quick overview for getting started with AIMind (MindsDB) [chat models](/docs/concepts/#chat-models). \n",
    "\n",
    "The [Minds Endpoint](https://docs.mdb.ai/) is a unified, OpenAI-compatible API format to interact with various Minds and LLM providers. This standardization simplifies your access to a wide range of functions like completion, embedding, and image generation. It minimizes the learning curve and accelerates your development process.\n",
    "\n",
    "It provides a wide selection of [advanced large language models (LLMs)](https://docs.mdb.ai/docs/models) from industry-leading AI providers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d1222-74f9-4ad2-8028-bceb9b19f177",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To access MindsDB Endpoint models you'll need to create an MindsDB account, get an API key, and install the `openai` package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Head to https://mdb.ai/ to sign up to MindsDB and generate an API key. Once you've done this set the `MINDSDB_API_KEY` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfb1ccf-4fa2-42fc-8b78-ab20c1ce78ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your MINDSDB API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"MINDSDB_API_KEY\"] = getpass.getpass(\"Enter your MINDSDB API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fec8f0-8962-4e38-8e54-1e6e8fd106ef",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2430b258-2d73-473e-a15d-a202b79909bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56601109-77a8-4998-a34c-8230936c2be7",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain AIMind (MindsDB) integration lives in the `langchain-community` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b33c0-e548-4e18-9d7b-3ca66d4db709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f5a3d-d526-491c-a3c3-2dc698a2beef",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fb985f-2f98-4392-a829-b72bdd46fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAIMind\n",
    "\n",
    "llm = ChatAIMind(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    # mindsdb_api_key=\"...\",  # if you prefer to pass api key in directly instead of using env vars\n",
    "    # mindsdb_base_url=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561ff50-751b-426a-ad9c-784ab63e02d1",
   "metadata": {},
   "source": [
    "## Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4383f5b-b89a-42a1-80c9-c0ac469b0a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Me encanta programar.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 31, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ec11c69-985d-44e1-8b4f-ade22ffdfff0-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to Spanish. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edef66c-449a-4bdc-aa3d-09cfcf3ed3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me encanta programar.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02436eb8-6ee6-4438-b69e-b304c7d8ef28",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e051ea-c352-4a9f-aadf-8164539e64b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Adoro programmare.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 26, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-50c6faf0-47b7-42b4-8a52-4f9c42ab09e4-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Italian\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52212515-e895-4611-ad86-c10feb68dc3a",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "\n",
    "AIMind also supports [tool calling](https://platform.openai.com/docs/guides/function-calling) (we use \"tool calling\" and \"function calling\" interchangeably here) for several models that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.\n",
    "\n",
    "Given below is a list of models that support tool calling:\n",
    "- gpt-3.5-turbo\n",
    "- claude-3-haiku\n",
    "- firefunction-v1\n",
    "- hermes-2-pro\n",
    "- mistral-7b\n",
    "- mixtral-8x7b\n",
    "- gemini-1.5-pro\n",
    "\n",
    "### ChatAIMind.bind_tools()\n",
    "\n",
    "With `ChatAIMind.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to an OpenAI tool schemas, which looks like:\n",
    "```\n",
    "{\n",
    "    \"name\": \"...\",\n",
    "    \"description\": \"...\",\n",
    "    \"parameters\": {...}  # JSONSchema\n",
    "}\n",
    "```\n",
    "and passed in every model invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e74a06-6296-4a83-87a7-b01adcd2010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([GetWeather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f36d340-bd86-424d-bee0-4cc166643979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W2Td6ajWeeM3TtRAKuapX6GH', 'function': {'arguments': '{\"location\":\"San Francisco\"}', 'name': 'GetWeather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 70, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-40bb083c-37fa-450b-90eb-bb6786c73443-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco'}, 'id': 'call_W2Td6ajWeeM3TtRAKuapX6GH'}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\n",
    "    \"what is the weather like in San Francisco\",\n",
    ")\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50ae18-4f87-4c4d-be4f-796e2f3b1bc0",
   "metadata": {},
   "source": [
    "### AIMessage.tool_calls\n",
    "Notice that the AIMessage has a `tool_calls` attribute. This contains in a standardized ToolCall format that is model-provider agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21f16c8-5c7d-4fa2-8363-f5c6dc3c6601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetWeather',\n",
       "  'args': {'location': 'San Francisco'},\n",
       "  'id': 'call_W2Td6ajWeeM3TtRAKuapX6GH'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271b837-a06e-4885-9037-d8d3e034937a",
   "metadata": {},
   "source": [
    "For more on binding tools and tool call outputs, head to the [tool calling](/docs/how_to/function_calling) docs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
