{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis\n",
    "\n",
    ">[Redis vector database](https://redis.io/docs/get-started/vector-database/) introduction and langchain integration guide.\n",
    "\n",
    "## What is Redis?\n",
    "\n",
    "Most developers from a web services background are familiar with `Redis`. At its core, `Redis` is an open-source key-value store that is used as a cache, message broker, and database. Developers choose `Redis` because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years.\n",
    "\n",
    "On top of these traditional use cases, `Redis` provides additional capabilities like the Search and Query capability that allows users to create secondary index structures within `Redis`. This allows `Redis` to be a Vector Database, at the speed of a cache. \n",
    "\n",
    "\n",
    "## Redis as a Vector Database\n",
    "\n",
    "`Redis` uses compressed, inverted indexes for fast indexing with a low memory footprint. It also supports a number of advanced features such as:\n",
    "\n",
    "* Indexing of multiple fields in Redis hashes and `JSON`\n",
    "* Vector similarity search (with `HNSW` (ANN) or `FLAT` (KNN))\n",
    "* Vector Range Search (e.g. find all vectors within a radius of a query vector)\n",
    "* Incremental indexing without performance loss\n",
    "* Document ranking (using [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), with optional user-provided weights)\n",
    "* Field weighting\n",
    "* Complex boolean queries with `AND`, `OR`, and `NOT` operators\n",
    "* Prefix matching, fuzzy matching, and exact-phrase queries\n",
    "* Support for [double-metaphone phonetic matching](https://redis.io/docs/stack/search/reference/phonetic_matching/)\n",
    "* Auto-complete suggestions (with fuzzy prefix suggestions)\n",
    "* Stemming-based query expansion in [many languages](https://redis.io/docs/stack/search/reference/stemming/) (using [Snowball](http://snowballstem.org/))\n",
    "* Support for Chinese-language tokenization and querying (using [Friso](https://github.com/lionsoul2014/friso))\n",
    "* Numeric filters and ranges\n",
    "* Geospatial searches using Redis geospatial indexing\n",
    "* A powerful aggregations engine\n",
    "* Supports for all `utf-8` encoded text\n",
    "* Retrieve full documents, selected fields, or only the document IDs\n",
    "* Sorting results (for example, by creation date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedisVectorStore\n",
    "\n",
    "This notebook demonstrates the usage of RedisVectorStore from the langchain-redis partner package. RedisVectorStore leverages Redis as a vector database, enabling efficient storage, retrieval, and similarity search of vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, we need to install the necessary packages. Run the following command to install langchain-redis, sentence-transformers, and scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-redis langchain-huggingface sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "We'll import the necessary libraries for our tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_redis import RedisVectorStore\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Redis Connection\n",
    "To use RedisVectorStore, you need a running Redis instance. For this example, we assume a local Redis instance running on the default port. Modify the URL if your setup differs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://redis:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that Redis is up an running by pinging it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "redis_client = redis.from_url(REDIS_URL)\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Sample Data\n",
    "We'll use a subset of the 20 Newsgroups dataset for this demonstration. This dataset contains newsgroup posts on various topics. We'll focus on two categories: 'alt.atheism' and 'sci.space':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\"alt.atheism\", \"sci.space\"]\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Use only the first 250 documents\n",
    "texts = newsgroups.data[:250]\n",
    "metadata = [\n",
    "    {\"category\": newsgroups.target_names[target]} for target in newsgroups.target[:250]\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadata)\n",
    "]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'category': 'alt.atheism'}, page_content='From: bil@okcforum.osrhe.edu (Bill Conner)\\nSubject: Re: Not the Omni!\\nNntp-Posting-Host: okcforum.osrhe.edu\\nOrganization: Okcforum Unix Users Group\\nX-Newsreader: TIN [version 1.1 PL6]\\nLines: 18\\n\\nCharley Wingate (mangoe@cs.umd.edu) wrote:\\n: \\n: >> Please enlighten me.  How is omnipotence contradictory?\\n: \\n: >By definition, all that can occur in the universe is governed by the rules\\n: >of nature. Thus god cannot break them. Anything that god does must be allowed\\n: >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\\n: >the rules of nature.\\n: \\n: Obviously, an omnipotent god can change the rules.\\n\\nWhen you say, \"By definition\", what exactly is being defined;\\ncertainly not omnipotence. You seem to be saying that the \"rules of\\nnature\" are pre-existant somehow, that they not only define nature but\\nactually cause it. If that\\'s what you mean I\\'d like to hear your\\nfurther thoughts on the question.\\n\\nBill\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "We'll use the SentenceTransformer model to create embeddings. This model runs locally and doesn't require an API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage with LangChain's RedisVectorStore\n",
    "Now we'll demonstrate basic usage of RedisVectorStore, including creating an instance, inserting data, and performing a simple similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a RedisVectorStore instance and inserting data\n",
    "We'll create a RedisVectorStore instance and populate it with our sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = RedisVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    redis_url=REDIS_URL,\n",
    "    index_name=\"newsgroups\",\n",
    "    metadata_schema=[\n",
    "        {\"name\": \"category\", \"type\": \"tag\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a simple similarity search\n",
    "Let's perform a basic similarity search using a query about space exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Similarity Search Results:\n",
      "Content: From: aa429@freenet.carleton.ca (Terry Ford)\n",
      "Subject: A flawed propulsion system: Space Shuttle\n",
      "X-Ad...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n",
      "Content: From: nsmca@aurora.alaska.edu\n",
      "Subject: Space Design Movies?\n",
      "Article-I.D.: aurora.1993Apr23.124722.1\n",
      "...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about space exploration\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Simple Similarity Search Results:\")\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Queries with RedisVectorStore\n",
    "RedisVectorStore supports more advanced query types. We'll demonstrate similarity search with metadata filtering, maximum marginal relevance search, and similarity search with score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search with metadata filtering\n",
    "We can filter our search results based on metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Similarity Search Results:\n",
      "Content: From: aa429@freenet.carleton.ca (Terry Ford)\n",
      "Subject: A flawed propulsion system: Space Shuttle\n",
      "X-Ad...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n",
      "Content: From: nsmca@aurora.alaska.edu\n",
      "Subject: Space Design Movies?\n",
      "Article-I.D.: aurora.1993Apr23.124722.1\n",
      "...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from redisvl.query.filter import Tag\n",
    "\n",
    "query = \"Tell me about space exploration\"\n",
    "\n",
    "# Create a filter expression\n",
    "filter_condition = Tag(\"category\") == \"sci.space\"\n",
    "\n",
    "filtered_results = vector_store.similarity_search(query, k=2, filter=filter_condition)\n",
    "\n",
    "print(\"Filtered Similarity Search Results:\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance search\n",
    "Maximum marginal relevance search helps in getting diverse results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Marginal Relevance Search Results:\n",
      "Content: From: aa429@freenet.carleton.ca (Terry Ford)\n",
      "Subject: A flawed propulsion system: Space Shuttle\n",
      "X-Ad...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n",
      "Content: From: moroney@world.std.com (Michael Moroney)\n",
      "Subject: Re: Vulcan? (No, not the guy with the ears!)\n",
      "...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximum marginal relevance search with filter\n",
    "mmr_results = vector_store.max_marginal_relevance_search(\n",
    "    query, k=2, fetch_k=10, filter=filter_condition\n",
    ")\n",
    "\n",
    "print(\"Maximum Marginal Relevance Search Results:\")\n",
    "for doc in mmr_results:\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search with score\n",
    "We can also get similarity scores along with our search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Search with Score Results:\n",
      "Content: From: aa429@freenet.carleton.ca (Terry Ford)\n",
      "Subject: A flawed propulsion system: Space Shuttle\n",
      "X-Ad...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "Score: 0.569670796394\n",
      "\n",
      "Content: From: nsmca@aurora.alaska.edu\n",
      "Subject: Space Design Movies?\n",
      "Article-I.D.: aurora.1993Apr23.124722.1\n",
      "...\n",
      "Metadata: {'category': 'sci.space'}\n",
      "Score: 0.590400338173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarity search with score and filter\n",
    "scored_results = vector_store.similarity_search_with_score(\n",
    "    query, k=2, filter=filter_condition\n",
    ")\n",
    "\n",
    "print(\"Similarity Search with Score Results:\")\n",
    "for doc, score in scored_results:\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "After we're done, it's important to clean up our Redis indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the underlying index and it's data\n",
    "vector_store.index.delete(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Redis Kitchen Sink Example\n",
    "\n",
    "This portion of the notebook demonstrates a comprehensive example that combines RedisVectorStore, RedisCache, and RedisChatMessageHistory to create a powerful, efficient, and context-aware chatbot system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-openai wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have a Redis server running. You can start one using Docker with:\n",
    "\n",
    "```\n",
    "docker run -d -p 6379:6379 redis:latest\n",
    "```\n",
    "\n",
    "Or install and run Redis locally according to your operating system's instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://redis:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_redis import RedisCache, RedisChatMessageHistory, RedisVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Check if OPENAI_API_KEY is already set in the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API key not found in environment variables.\")\n",
    "    openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "    # Set the API key for the current session\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"OpenAI API key has been set for this session.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Index with RedisVL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll set up our vector store using RedisVL, which provides a powerful interface for creating and managing vector indexes in Redis. We'll define a schema for our Wikipedia data, create an index using RedisVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_redis import RedisConfig, RedisVectorStore\n",
    "from redis import Redis\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema import IndexSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RedisVL Index Schema\n",
    "\n",
    "We start by defining a schema for our index. This schema includes:\n",
    "- A text field for the document content\n",
    "- A text field for metadata\n",
    "- A vector field for the document embeddings\n",
    "\n",
    "The vector field is configured with 1536 dimensions (suitable for OpenAI embeddings), using cosine distance and a FLAT index algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\n",
    "            \"name\": \"kitchensink_docs\",\n",
    "            \"storage_type\": \"hash\",\n",
    "            \"prefix\": \"wiki:\",\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"text\", \"type\": \"text\"},\n",
    "            {\"name\": \"url\", \"type\": \"tag\"},\n",
    "            {\"name\": \"title\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 1536,\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                    \"algorithm\": \"FLAT\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RedisVL Index\n",
    "\n",
    "Using the defined schema, we create a SearchIndex object and use it to create the actual index in Redis. This step sets up the structure that our vector store will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index using RedisVL\n",
    "redisvl_index = SearchIndex(schema, redis_client)\n",
    "redisvl_index.create(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing RedisVectorStore\n",
    "\n",
    "With the RedisVL index in place, we can now initialize our RedisVectorStore. We use a RedisConfig object to specify the index name and Redis URL, ensuring that our vector store connects to the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:12:08 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisVectorStore using the pre-constructed index\n",
    "config = RedisConfig(\n",
    "    index_name=\"kitchensink_docs\", redis_url=REDIS_URL, from_existing=True\n",
    ")\n",
    "vector_store = RedisVectorStore(OpenAIEmbeddings(), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Components\n",
    "\n",
    "We also initialize other components like RedisCache for LLM caching, ChatOpenAI for our language model, and RedisChatMessageHistory for maintaining conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# Initialize ChatOpenAI with caching\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Initialize RedisChatMessageHistory\n",
    "message_history = RedisChatMessageHistory(\"kitchensink_chat\", redis_url=REDIS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Vector Store with Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 439 document chunks to the vector store.\n"
     ]
    }
   ],
   "source": [
    "## Populate Vector Store with Wikipedia Data\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def fetch_wikipedia_content(titles):\n",
    "    documents = []\n",
    "    for title in titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            doc = Document(\n",
    "                page_content=page.content, metadata={\"title\": title, \"url\": page.url}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # Choose the first option from the disambiguation list\n",
    "            page = wikipedia.page(e.options[0])\n",
    "            doc = Document(\n",
    "                page_content=page.content,\n",
    "                metadata={\"title\": e.options[0], \"url\": page.url},\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page not found for {title}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Fetch some Wikipedia articles\n",
    "titles = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Deep Learning\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Large Language Models\",\n",
    "    \"Robotics\",\n",
    "]\n",
    "documents = fetch_wikipedia_content(titles)\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_documents(splits)\n",
    "\n",
    "print(f\"Added {len(splits)} document chunks to the vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def combine_chat_history_and_question(inputs):\n",
    "    return f\"Chat History: {inputs['chat_history']}\\nHuman: {inputs['question']}\"\n",
    "\n",
    "\n",
    "# Update the prompt template to include chat history\n",
    "prompt_template = \"\"\"\n",
    "    You are an AI assistant answering questions based on the provided context and chat history. Be concise and accurate.\n",
    "\n",
    "    Context: {context}\n",
    "    {question}\n",
    "    AI Assistant:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        \"question\": combine_chat_history_and_question,\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Assistant! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  What are the core tenets of AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "The core tenets of AI include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. These are the traditional goals of AI research and are essential for creating intelligent machines that can perform tasks on par with humans. Additionally, AI also draws upon various fields such as psychology, linguistics, philosophy, neuroscience, and others to further advance its capabilities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  How does AI influence robotics, and viceversa?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "AI and robotics have a symbiotic relationship, as advancements in one field often lead to advancements in the other. AI influences robotics by providing the software and algorithms that enable robots to make decisions and learn from their environment. On the other hand, robotics influences AI by providing real-world data and challenges for AI systems to learn from and improve upon. Together, they are driving the development of advanced technologies that have the potential to greatly impact various industries and aspects of our daily lives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the AI Assistant!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "def get_chat_history(history):\n",
    "    return \"\\n\".join(\n",
    "        [f\"{msg.type.capitalize()}: {msg.content}\" for msg in history.messages[-5:]]\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Welcome to the AI Assistant! Type 'exit' to end the conversation.\")\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"Human: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user message to history\n",
    "    message_history.add_user_message(user_input)\n",
    "\n",
    "    # Get response from RAG chain\n",
    "    result = rag_chain.invoke(\n",
    "        {\"question\": user_input, \"chat_history\": get_chat_history(message_history)}\n",
    "    )\n",
    "\n",
    "    # Add AI message to history\n",
    "    message_history.add_ai_message(result)\n",
    "\n",
    "    print(f\"AI: {result}\")\n",
    "\n",
    "print(\"Thank you for using the AI Assistant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Kitchen Sink Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the integration of multiple Redis-based components in LangChain:\n",
    "\n",
    "1. **RedisVectorStore**: Used to store and retrieve document chunks from Wikipedia articles. It enables efficient similarity search for relevant context.\n",
    "\n",
    "2. **RedisCache**: Implemented to cache LLM responses, potentially speeding up repeated or similar queries.\n",
    "\n",
    "3. **RedisChatMessageHistory**: Stores the conversation history, allowing the AI to maintain context across multiple interactions.\n",
    "\n",
    "The combination of these components creates a powerful, context-aware chatbot system with the following features:\n",
    "\n",
    "- **Efficient Information Retrieval**: The vector store allows quick access to relevant information from a large dataset.\n",
    "- **Improved Response Time**: Caching helps in reducing API calls for similar or repeated queries.\n",
    "- **Contextual Understanding**: The chat history enables the AI to reference previous parts of the conversation.\n",
    "- **Scalability**: Redis as a backend allows this system to handle large amounts of data and high traffic efficiently.\n",
    "\n",
    "This kitchen sink example showcases how these Redis-based components can work together seamlessly in a real-world application, demonstrating the power and flexibility of the langchain-redis package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "# Clear vector store\n",
    "vector_store.index.delete(drop=True)\n",
    "\n",
    "# Clear cache\n",
    "redis_cache.clear()\n",
    "\n",
    "# Clear chat history\n",
    "message_history.clear()\n",
    "\n",
    "print(\"Cleanup completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
